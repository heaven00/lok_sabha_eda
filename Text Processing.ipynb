{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open('data/12.08.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_stopwords = ['versions',\n",
    "                     'hindi',\n",
    "                     'published',\n",
    "                     'laying',\n",
    "                     'mentioned',\n",
    "                     'papers',\n",
    "                     'august',\n",
    "                     'statement',\n",
    "                     'copy',\n",
    "                     'rules',\n",
    "                     'showing',\n",
    "                     'sebi',\n",
    "                     'delay',\n",
    "                     'regulations',\n",
    "                     'reasons',\n",
    "                     'english',\n",
    "                     'report',\n",
    "                     'government',\n",
    "                     'india',\n",
    "                     'notification',\n",
    "                     'amendment',\n",
    "                     'comptroller',\n",
    "                     'gazette',\n",
    "                     'ladnro',\n",
    "                     'union',\n",
    "                     'general',\n",
    "                     'auditor',\n",
    "                     'dated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sents = []\n",
    "for line in file.split('\\n'):\n",
    "    if line != \"\":\n",
    "        pattern = re.compile('[a-zA-Z]*')\n",
    "        words = [word for word in pattern.findall(line) if len(word) > 3 and \n",
    "                                                           word.lower() not in stopwords.words('english') + custom_stopwords]\n",
    "        if len(words) > 4:\n",
    "            sents.append([word.lower() for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_frequency(sents):\n",
    "    word_counter = {}\n",
    "    for sent in sents:\n",
    "        for word in sent:\n",
    "            try:\n",
    "                word_counter[word] += 1\n",
    "            except:\n",
    "                word_counter[word] = 1\n",
    "    return word_counter\n",
    "\n",
    "word_freq = get_word_frequency(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shri', 38),\n",
       " ('securities', 35),\n",
       " ('exchange', 33),\n",
       " ('minister', 32),\n",
       " ('board', 32),\n",
       " ('aircraft', 28),\n",
       " ('year', 25),\n",
       " ('madam', 22),\n",
       " ('ministry', 19),\n",
       " ('indian', 18)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kw_pair = []\n",
    "for key in sorted(word_freq, key=word_freq.__getitem__, reverse=True):\n",
    "    kw_pair.append((key, word_freq[key]))\n",
    "kw_pair[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('exchange', 'board'), 30),\n",
       " (('securities', 'exchange'), 30),\n",
       " (('disclosure', 'requirements'), 11),\n",
       " (('would', 'like'), 11),\n",
       " (('issue', 'capital'), 8),\n",
       " (('capital', 'disclosure'), 8),\n",
       " (('board', 'issue'), 7),\n",
       " (('corporation', 'limited'), 6),\n",
       " (('review', 'working'), 6),\n",
       " (('state', 'ministry'), 6)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgs = nltk.ngrams([word for sent in sents for word in sent], 2)\n",
    "fdist = nltk.FreqDist(bgs)\n",
    "fdist.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('exchange', 'board'),\n",
       " ('securities', 'exchange'),\n",
       " ('disclosure', 'requirements'),\n",
       " ('would', 'like'),\n",
       " ('review', 'working'),\n",
       " ('capital', 'disclosure'),\n",
       " ('list', 'essential'),\n",
       " ('renewal', 'recognition'),\n",
       " ('issue', 'capital'),\n",
       " ('alongwith', 'audited')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try out collocations\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_words([word for sent in sents for word in sent])\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accepted', 'recommendation'),\n",
       " ('achievements', 'beneficial'),\n",
       " ('adding', 'either'),\n",
       " ('addition', 'bombardier'),\n",
       " ('addressed', 'challenged'),\n",
       " ('administrative', 'functioning'),\n",
       " ('adopt', 'updated'),\n",
       " ('adulterated', 'spoiled'),\n",
       " ('advantage', 'loopholes'),\n",
       " ('afford', 'costly')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finder.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('securities', 'exchange', 'board'),\n",
       " ('exchange', 'board', 'issue'),\n",
       " ('exchange', 'board', 'substantial'),\n",
       " ('exchange', 'board', 'depositories'),\n",
       " ('exchange', 'board', 'mumbai'),\n",
       " ('exchange', 'board', 'listing'),\n",
       " ('exchange', 'board', 'delisting'),\n",
       " ('exchange', 'board', 'securities'),\n",
       " ('exchange', 'board', 'intermediaries'),\n",
       " ('exchange', 'board', 'mutual')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words([word for sent in sents for word in sent])\n",
    "finder.nbest(trigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "num_topics = 10\n",
    "clf = decomposition.NMF(n_components=num_topics, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit_transform in module sklearn.decomposition.nmf:\n",
      "\n",
      "fit_transform(X, y=None, W=None, H=None) method of sklearn.decomposition.nmf.NMF instance\n",
      "    Learn a NMF model for the data X and returns the transformed data.\n",
      "    \n",
      "    This is more efficient than calling fit followed by transform.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X: {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "        Data matrix to be decomposed\n",
      "    \n",
      "    W : array-like, shape (n_samples, n_components)\n",
      "        If init='custom', it is used as initial guess for the solution.\n",
      "    \n",
      "    H : array-like, shape (n_components, n_features)\n",
      "        If init='custom', it is used as initial guess for the solution.\n",
      "    \n",
      "    Attributes\n",
      "    ----------\n",
      "    components_ : array-like, shape (n_components, n_features)\n",
      "        Factorization matrix, sometimes called 'dictionary'.\n",
      "    \n",
      "    n_iter_ : int\n",
      "        Actual number of iterations for the transform.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    W: array, shape (n_samples, n_components)\n",
      "        Transformed data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(clf.fit_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = [word for sent in sents for word in sent]\n",
    "sent2vec = count_vectorizer.fit_transform([str(' '.join(sent)) for sent in sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.corpus import brown\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from sklearn.pipeline import FeatureUnion\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# # Let's get more text from NLTK\n",
    "# text = [\" \".join(i) for i in brown.sents()[:100]]\n",
    "# # I'm just gonna assign random tags.\n",
    "# labels = ['yes']*50 + ['no']*50\n",
    "# count_vectorizer = CountVectorizer(stop_words=\"english\", min_df=3)\n",
    "# tf_transformer = TfidfVectorizer(use_idf=True)\n",
    "# combined_features = FeatureUnion([(\"counts\", count_vectorizer), (\"tfidf\", tf_transformer)]).fit_transform(text)\n",
    "# classifier = MultinomialNB()\n",
    "# classifier.fit(combined_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = pd.np.array(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  2.06348584e-02,   2.41431151e-03,   7.75545783e-02, ...,\n",
       "          1.82550893e-01,   7.81280366e-02,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          0.00000000e+00,   7.54152828e-02,   0.00000000e+00],\n",
       "       ..., \n",
       "       [  2.03679735e-02,   0.00000000e+00,   1.62521593e-02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.24134082e-02],\n",
       "       [  6.31347664e-05,   5.37454014e-03,   0.00000000e+00, ...,\n",
       "          1.77491042e-03,   3.03745227e-03,   1.27721688e-03],\n",
       "       [  2.37185588e-02,   0.00000000e+00,   2.01254362e-02, ...,\n",
       "          0.00000000e+00,   0.00000000e+00,   1.35589001e-02]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit_transform(sent2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = pd.np.argsort(topic)[::-1][0:5]\n",
    "    topic_words.append([vocab[i] for i in word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['banks', 'whether', 'taken', 'public', 'shri'],\n",
       " ['aircraft', 'indian', 'deployed', 'three', 'search'],\n",
       " ['milk', 'technology', 'products', 'ministry', 'food'],\n",
       " ['minister', 'start', 'first', 'startups', 'capital'],\n",
       " ['national', 'list', 'essential', 'devices', 'medical'],\n",
       " ['securities', 'exchange', 'board', 'issue', 'requirements'],\n",
       " ['radar', 'flight', 'july', 'port', 'blair'],\n",
       " ['compulsory', 'cadets', 'member', 'come', 'lakh'],\n",
       " ['years', 'accidents', 'figure', 'accident', 'committee'],\n",
       " ['prices', 'stent', 'come', 'cent', 'going']]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF seems to work quite better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
